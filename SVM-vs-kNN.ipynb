{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM vs. kNN Classification\n",
    "\n",
    "#### In this project, I have used classification methods to classify handwritten digits. Specifically, I have compared the performance and accuracy of using a Support Vector Machine (SVM) vs. k Nearest Neighbors (kNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import tree, svm, metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, KFold\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE MNIST handwritten digit dataset consists of images of handwritten digits, together with labels indicating which digit is in each image. There are several versions of the MNIST dataset. I have used the one that is built into scikit-learn. I have scaled the data before running them through my algorithms, which altered their appearance when I plotted them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to scale images...\n",
    "\n",
    "digits = load_digits()\n",
    "X = scale( digits.data )\n",
    "y = digits.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "n_digits = len( np.unique(digits.target) )\n",
    "\n",
    "print( \"n_digits: %d, n_samples %d, n_features %d\" % (n_digits, n_samples, n_features) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what one digit (the \"zero\") looks like:\n",
    "\n",
    "print( \"===\\nThe raw data\" )\n",
    "print( digits.images[0] )\n",
    "\n",
    "print( \"===\\nThe scaled data\" )\n",
    "print( X[0] )\n",
    "\n",
    "print( \"===\\nThe digit\" )\n",
    "print( digits.target[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 25 images...\n",
    "\n",
    "plt.figure( figsize=(10, 10) )\n",
    " \n",
    "for ii in np.arange( 25 ):\n",
    "    plt.subplot( 5, 5, ii+1 )\n",
    "    plt.imshow( np.reshape( X[ii,:], (8,8) ), cmap='Greys', interpolation='nearest' )\n",
    "    plt.axis( 'off' )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training and test sets\n",
    "\n",
    "XTrain, XTest, yTrain, yTest = train_test_split( X, y, random_state=1, test_size=0.8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SVM with an rbf kernel and the cost parameter C=5 to build a classifier using the training dataset.\n",
    "model = svm.SVC(kernel='rbf', C=5)\n",
    "model.fit(XTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the test dataset, evaluate the accuracy of the model.\n",
    "y_pred = model.predict(XTest)\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again using the test dataset, compute the confusion matrix.\n",
    "print(metrics.confusion_matrix(y_true = yTest, y_pred = y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all of the misclassified digits as images (title with: Predicted #, Actual #).\n",
    "plt.figure( figsize=(12, 12) )\n",
    "misclassified_count = 0\n",
    "\n",
    "for i in range(len(yTest)):\n",
    "    if y_pred[i] != yTest[i]:\n",
    "        misclassified_count += 1\n",
    "        plt.subplot(2, 4, misclassified_count)\n",
    "        plt.imshow(np.reshape(XTest[i], (8, 8)), cmap='Greys', interpolation='nearest')\n",
    "        plt.title(f\"Predicted {y_pred[i]}\\nActual {yTest[i]}\", fontsize=10)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if misclassified_count == 8:\n",
    "            break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the 'cross_val_score' function, evaluate the accuracy of the SVM for different values\n",
    "# of the parameter C: .5 to 5 (by .1) and then 10-50 (by 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of C values to test\n",
    "c_values = np.concatenate((np.arange(0.5, 5.1, 0.1), np.arange(10, 51, 20)))\n",
    "\n",
    "# Initialize a list to store the mean accuracies\n",
    "mean_accuracies = []\n",
    "\n",
    "# Loop over the range of C values\n",
    "for c in c_values:\n",
    "    # Create the SVM model with the current C value\n",
    "    model = svm.SVC(kernel='rbf', C=c)\n",
    "    \n",
    "    # Perform cross-validation with 5 folds and get the accuracy scores\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    \n",
    "    # Calculate the mean accuracy and append it to the list\n",
    "    mean_accuracy = scores.mean()\n",
    "    mean_accuracies.append(mean_accuracy)\n",
    "\n",
    "#mean_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(c_values, mean_accuracies, marker='o', linestyle='-')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.title('SVM Accuracy vs. C')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the best value of C that gives the highest mean accuracy\n",
    "best_c = c_values[np.argmax(mean_accuracies)]\n",
    "best_accuracy = np.max(mean_accuracies)\n",
    "\n",
    "print(f\"Best value of C: {best_c}\")\n",
    "print(f\"Highest mean accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = digits.data\n",
    "y_raw = digits.target\n",
    "\n",
    "# Split the data into a training and test sets on the raw data\n",
    "XTrain_raw, XTest_raw, yTrain, yTest = train_test_split(X_raw, y_raw, random_state=1, test_size=0.8)\n",
    "\n",
    "# Use SVM with an rbf kernel and the cost parameter C=5 to build a classifier using the training dataset.\n",
    "model_raw = svm.SVC(kernel='rbf', C=5)\n",
    "model_raw.fit(XTrain_raw, yTrain)\n",
    "\n",
    "# Using the test dataset, evaluate the accuracy of the model on raw data.\n",
    "y_pred_raw = model_raw.predict(XTest_raw)\n",
    "accuracy_raw = metrics.accuracy_score(y_true=yTest, y_pred=y_pred_raw)\n",
    "\n",
    "print('Accuracy on raw data =', accuracy_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy Score:** 0.975"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with K-nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training and test set\n",
    "XTrain, XTest, yTrain, yTest = train_test_split( X, y, random_state=1, test_size=0.8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use KNN Classification with k=10 to build a classifier using the training dataset.\n",
    "knn_model = KNeighborsClassifier(n_neighbors=10)\n",
    "knn_model.fit(XTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the test dataset, evaluate the accuracy of the model.\n",
    "y_pred_knn = knn_model.predict(XTest)\n",
    "accuracy_knn = metrics.accuracy_score(y_true=yTest, y_pred=y_pred_knn)\n",
    "\n",
    "print('Accuracy of k-NN (k=10):', accuracy_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again using the test dataset, compute the confusion matrix.\n",
    "confusion_matrix_knn = metrics.confusion_matrix(y_true=yTest, y_pred=y_pred_knn)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The most common mistake the classifier makes is misclassifying a 1 as a 9 (10 times). Overall, also, 1 was misclassified the most (26 times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all of the misclassified digits as images (title with: Predicted #, Actual #).\n",
    "plt.figure(figsize=(12, 8))\n",
    "misclassified_count_knn = 0\n",
    "\n",
    "for i in range(len(yTest)):\n",
    "    if y_pred_knn[i] != yTest[i]:\n",
    "        misclassified_count_knn += 1\n",
    "        plt.subplot(3, 4, misclassified_count_knn)\n",
    "        plt.imshow(np.reshape(XTest[i], (8, 8)), cmap='Greys', interpolation='nearest')\n",
    "        plt.title(f\"Predicted {y_pred_knn[i]}\\nActual {yTest[i]}\", fontsize=10)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if misclassified_count_knn == 12:\n",
    "            break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the 'cross_val_score' function, evaluate the accuracy of the KNN for different values \n",
    "# of the parameter k, first as k = 10\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = np.concatenate((np.arange(1, 11), np.arange(10, 51, 5)))\n",
    "\n",
    "# Initialize a list to store the mean accuracies\n",
    "mean_accuracies_knn = []\n",
    "\n",
    "# Loop over the range of k values\n",
    "for k in k_values:\n",
    "    # Create the k-NN model with the current k value\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    # Perform cross-validation with 5 folds and get the accuracy scores\n",
    "    scores = cross_val_score(knn_model, X, y, cv=5)\n",
    "    \n",
    "    # Calculate the mean accuracy and append it to the list\n",
    "    mean_accuracy = scores.mean()\n",
    "    mean_accuracies_knn.append(mean_accuracy)\n",
    "\n",
    "mean_accuracies_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph results\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, mean_accuracies_knn, marker='o', linestyle='-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.title('k-NN Accuracy vs. k')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the best value of k that gives the highest mean accuracy\n",
    "best_k = k_values[np.argmax(mean_accuracies_knn)]\n",
    "best_accuracy_knn = np.max(mean_accuracies_knn)\n",
    "\n",
    "print(f\"Best value of k for k-NN: {best_k}\")\n",
    "print(f\"Highest mean accuracy for k-NN: {best_accuracy_knn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The best value for k is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test the algorithm on the raw (non-scaled) data.\n",
    "# Split the data into a training and test set\n",
    "XTrain_raw, XTest_raw, yTrain_raw, yTest_raw = train_test_split(X_raw, y_raw, random_state=1, test_size=0.8)\n",
    "\n",
    "# Use k-NN with k=10 to build a classifier using the training dataset.\n",
    "model_raw = KNeighborsClassifier(n_neighbors=10)\n",
    "model_raw.fit(XTrain_raw, yTrain)\n",
    "\n",
    "# Using the test dataset, evaluate the accuracy of the model on raw data.\n",
    "y_pred_raw = model_raw.predict(XTest_raw)\n",
    "accuracy_raw = metrics.accuracy_score(y_true=yTest, y_pred=y_pred_raw)\n",
    "\n",
    "print('Accuracy on raw data =', accuracy_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:** In SVM, increasing the value of C worked in its favor up to a limit with smaller datasets. In large datasets, it worked well to have a high value of C. SVM did extremely well with 97% accuracy on the raw digit data.\n",
    "\n",
    "For kNN, the lower the number of dimensions and the smaller the dataset, the more accurate it was. It did well in the digit dataset with 94% accuracy on the raw data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
